<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>VitalGuard AI – Columbia University EECS E4764 IoT Project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
		<div id="header">

			<div class="top">

				<!-- Logo -->
				<div id="logo">
					<!-- <span class="image avatar48"><img src="images/avatar.jpg" alt="" /></span> -->
					<h1 id="title">VitalGuard AI</h1>
					<p>Columbia University <br>
						EECS E4764 Fall '25 <br>
						Artificial Intelligence of Things<br>
						Team 19 Project Report
					</p>
				</div>

				<!-- Nav -->
				<nav id="nav">
					<ul>
						<li><a href="#top" id="top-link" class="skel-layers-ignoreHref"><span class="icon fa-home">Abstract</span></a></li>
						<li><a href="#motivation" id="motivation-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Motivation</span></a></li>
						<li><a href="#system" id="system-link" class="skel-layers-ignoreHref"><span class="icon fa-th">System</span></a></li>
						<li><a href="#results" id="results-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Results</span></a></li>
						<li><a href="#references" id="references-link" class="skel-layers-ignoreHref"><span class="icon fa-th">References</span></a></li>
						<li><a href="#team" id="team-link" class="skel-layers-ignoreHref"><span class="icon fa-user">Our Team</span></a></li>
                        <li><a href="/ui" id="ui-link"><span class="icon fa-desktop">Live Web UI</span></a></li>
						<li><a href="#contact" id="contact-link" class="skel-layers-ignoreHref"><span class="icon fa-envelope">Contact</span></a></li>
					</ul>
				</nav>

			</div>

			<div class="bottom">

				<!-- Social Icons -->
				<ul class="icons">
					<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
					<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
                    <li>
                        <!-- Project GitHub repository link -->
                        <a href="https://github.com/JarrettChen217/EECS-E4764-2025-Fall-Final-Project-vitalguard-ai"
                           class="icon fa-github"
                           target="_blank"
                           rel="noopener noreferrer">
                            <span class="label">GitHub</span>
                        </a>
                    </li>
					<li><a href="#" class="icon fa-dribbble"><span class="label">Dribbble</span></a></li>
					<li><a href="#" class="icon fa-envelope"><span class="label">Email</span></a></li>
				</ul>

			</div>

		</div>

		<!-- Main -->
		<div id="main">

			<!-- Intro / Abstract -->
			<section id="top" class="one dark cover">
				<div class="container">

					<!-- TODO: Replace the src URL with your final demo / presentation video -->
					<iframe width="560" height="315"
						src="https://www.youtube.com/embed/b8C_5I-G9is?si=e3iXFLu5-OcrcuN3"
						title="VitalGuard AI Demo"
						frameborder="0"
						allowfullscreen></iframe>

					<h2 class="alt">VitalGuard AI: An AIoT Wearable for Real‑Time Health Insights &amp; Alerts</h2>

					<p>
						VitalGuard AI is a compact wearable AIoT system that continuously fuses multi-sensor vital signs
						(e.g., heart rate, SpO₂, temperature, activity, and stress) on an ESP32-based edge platform and
						a cloud backend to provide personalized health insights and trigger emergency alerts in real time.
					</p>

					<p>
						In the fields of chronic disease management and elderly care, continuous and low-cost monitoring
						outside clinical settings is crucial. By combining embedded sensing, cloud analytics, and
						large language models (LLMs), VitalGuard AI helps users track long-term health trends, receive
						actionable lifestyle suggestions (such as hydration, rest, and medication timing cues), and
						automatically notify caregivers or emergency services when dangerous anomalies or falls are detected.
					</p>

					<footer>
<!--						<a href="#motivation" class="button scrolly">Motivation</a>-->
                        <a href="/ui" class="button scrolly">User Web UI</a>
					</footer>

				</div>
			</section>

			<!-- Motivation -->
			<section id="motivation" class="two">
				<div class="container">

					<header>
						<h2>Motivation</h2>
					</header>

					<p align="left">
						Chronic diseases and aging populations are driving a strong need for continuous, low-cost
						health monitoring outside traditional clinical environments. Conventional check-ups are
						infrequent and often miss early warning signs, while hospital-grade monitoring is expensive
						and not scalable for daily life.
					</p>

					<p align="left">
						<b>Relevance.</b> Our goal is to enable out-of-clinic monitoring for:
						
					</p>
					<ul align="left">
						<li>Chronic disease management (e.g., cardiovascular conditions).</li>
						<li>Elderly care and safer independent living.</li>
						<li>High-stress or high-risk environments (e.g., campus safety, shift workers).</li>
					</ul>

					<p align="left">
						<b>Social value.</b> By continuously tracking multi-modal signals, VitalGuard AI can:					
					<ul align="left">
						<li>Detect anomalies earlier, enabling timely medical attention.</li>
						<li>Provide peace of mind for families and caregivers through automatic alerts.</li>
						<li>Support safer independent living for older adults or vulnerable individuals.</li>
					</ul>

					<p align="left">
						<b>Utility.</b> Beyond raw numbers, we aim to provide:
					</p>
					<ul align="left">
						<li>Actionable lifestyle suggestions (hydration, rest, activity pacing, medication cues).</li>
						<li>Long-term trend tracking through a web dashboard and mobile-friendly interface.</li>
						<li>Automatic alerts on falls or abnormal vital patterns via SMS/email/notifications.</li>
					</ul>

					<p align="left">
						<b>Market potential.</b> The same platform can be adapted to:
					</p>
					<ul align="left">
						<li>Remote Patient Monitoring (RPM) pilots with healthcare providers.</li>
						<li>Wellness wearables and personal health tracking.</li>
						<li>Campus safety and corporate wellness programs.</li>
					</ul>
					<div style="display: flex; gap: 20px; justify-content: center;">
    
                    <a href="#" class="image fit">
                         <div class="img-box">
                         <img src="images/case1.jpg" alt="Case 1" />
                          </div>
                    </a>

                    <a href="#" class="image fit">
                         <div class="img-box">
                         <img src="images/case_senior_citizen.jpg" alt="Case 3" />
                         </div>
                         </a>

                    </div>

                    
					<!-- TODO: Optionally insert a figure or photo illustrating use cases -->
					<!-- Example:
					<a href="#" class="image fit"><img src="images/motivation_use_cases.jpg" alt="VitalGuard AI use cases" /></a>
					-->

				</div>
			</section>

			<!-- System -->
			<section id="system" class="three">
				<div class="container">

					<header>
						<h2>System</h2>
					</header>

					<p align="left">
						VitalGuard AI is an end-to-end AIoT system composed of a wearable hardware platform, an ESP32-based
						edge computing layer, Wi‑Fi/Bluetooth connectivity, a cloud backend on Google Cloud Platform (GCP),
						an AI (LLM) layer for health report generation, and a web dashboard for visualization.
					</p>

					<h3 align="left">Architecture</h3>

					<p align="left">
						The overall architecture follows a sensing &rarr; edge processing &rarr; cloud analytics &rarr;
						AI insights &rarr; user feedback loop:
					</p>

                    <a href="#" class="image fit"><img src="images/block_diagram.png" alt="VitalGuard AI block diagram" /></a>
					</p>
					<ul align="left">
						<li><b>Hardware / Sensing Layer.</b>
							A compact wearable integrates multiple sensors:
							<ul>
								<li>MAX86150 for heart rate, SpO₂, and ECG (integrated PPG+ECG, high SNR).</li>
								<li>TMP117 for high-accuracy body/skin temperature (±0.1&nbsp;°C, I²C).</li>
								<li>ADXL345 3-axis accelerometer for motion, posture, and fall detection.</li>
								<li>FSR402 force sensor for pressure/force-based contact and activity cues.</li>
								<a href="#" class="image fit"
                                  style="display: flex; justify-content: center; align-items: center;">
                                  <img src="images/circuit_diagram.jpg"
                                  alt="VitalGuard AI block diagram"
                                  style="max-width: 350px; height: auto;" />
                            </a>
							</ul>
							These sensors are interfaced with an ESP32 microcontroller.
						</li>

						<li><b>Edge Computing (ESP32 with MicroPython).</b>
							The ESP32 continuously acquires raw signals, performs basic pre-processing
							(e.g., filtering, feature extraction, normalization, threshold-based anomaly hints),
							and manages communication over Wi‑Fi and Bluetooth.
						</li>

						<li><b>Network Layer.</b>
							Data and extracted features are transmitted from the ESP32 to the cloud server in real time
							via Wi‑Fi. Depending on the scenario, MQTT, HTTP, or persistent sockets can be used.
						</li>

						<li><b>Cloud Backend (GCP + Flask).</b>
							A Python/Flask backend hosted on Google Cloud Platform receives streaming data through
							sockets or HTTP endpoints. A processing pipeline performs:
							<ul>
								<li>Data cleaning and resampling.</li>
								<li>Feature extraction (heart rate variability, activity levels, temperature trends).</li>
								<li>Normalization and aggregation into time windows.</li>
								<li>Rule-based and lightweight ML-based anomaly screening.</li>
							</ul>
							Processed data and events are stored for visualization and further analysis.
						</li>

						<li><b>Machine Learning–LLM Interface.</b>
						    <p align="left">
						        The Machine Learning–LLM interface forms the bridge between low-level physiological
						        signal processing and high-level, human-readable health insights. Traditional
						        machine learning and signal-processing techniques are used to transform raw,
						        multi-sensor time-series data into structured and interpretable health states,
						        which are then passed to a large language model (LLM) for contextual reasoning
						        and explanation.
						    </p>
						
						    <ul>
						        <li>
						            <b>ML-based feature extraction.</b>
						            Incoming sensor streams are segmented into sliding time windows, where
						            lightweight statistical and signal-derived features are computed, such as
						            mean heart rate and SpO₂, temperature trends, and accelerometer-based activity
						            metrics.
						        </li>
						
						        <li>
						            <b>Discrete health state classification.</b>
						            Extracted features are mapped to interpretable health categories using
						            rule-based and lightweight ML classifiers. Examples include heart rate
						            levels (low, normal, high), activity states (resting to vigorous activity),
						            temperature status (normal to elevated), and SpO₂ status (normal to low).
						            Simple heuristics also combine multiple signals to infer higher-level states
						            such as potential rest or sleep periods.
						        </li>
						
						        <li>
						            <b>Structured ML outputs.</b>
						            Rather than sending raw biometric time series to the LLM, only structured
						            outputs are forwarded, including discrete state labels, aggregated statistics,
						            and detected event flags (e.g., abnormal vitals or possible fall events).
						            This reduces noise, preserves privacy, and ensures that the LLM operates on
						            clinically meaningful abstractions.
						        </li>
						
						        <li>
						            <b>LLM-based reasoning and explanation.</b>
						            The LLM consumes the structured ML outputs and recent historical context to
						            generate natural-language health reports, contextual explanations, and
						            personalized lifestyle recommendations for users and caregivers.
						        </li>
						    </ul>
						
						    <p align="left">
						        In this hybrid design, the LLM does not replace ML-based detection or safety logic.
						        All critical anomaly detection and alert triggers remain governed by deterministic
						        rules or lightweight classifiers, while the LLM augments the system by translating
						        machine-level outputs into intuitive, actionable insights.
						    </p>
						</li>
						<svg width="700" height="320" viewBox="0 0 700 320" xmlns="http://www.w3.org/2000/svg">
						  <style>
						    .box {
						      fill: #f8f9fa;
						      stroke: #333;
						      stroke-width: 1.2;
						      rx: 6;
						      ry: 6;
						    }
						    .text {
						      font-family: Arial, sans-serif;
						      font-size: 13px;
						      fill: #000;
						      text-anchor: middle;
						    }
						    .arrow {
						      stroke: #333;
						      stroke-width: 1.5;
						      marker-end: url(#arrowhead);
						    }
						  </style>
						
						  <defs>
						    <marker id="arrowhead" markerWidth="10" markerHeight="7"
						            refX="10" refY="3.5" orient="auto">
						      <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
						    </marker>
						  </defs>
						
						  <!-- Boxes -->
						  <rect class="box" x="20"  y="40" width="200" height="50"/>
						  <rect class="box" x="250" y="40" width="200" height="50"/>
						  <rect class="box" x="480" y="40" width="200" height="50"/>
						
						  <rect class="box" x="135" y="130" width="200" height="50"/>
						  <rect class="box" x="365" y="130" width="200" height="50"/>
						
						  <!-- Text -->
						  <text class="text" x="120" y="70">Raw Sensor Data</text>
						  <text class="text" x="120" y="86">(HR, SpO₂, Temp, Motion)</text>
						
						  <text class="text" x="350" y="70">ML Feature Extraction</text>
						  <text class="text" x="350" y="86">(Windowing, Statistics)</text>
						
						  <text class="text" x="580" y="70">ML Classification</text>
						  <text class="text" x="580" y="86">(Health States & Flags)</text>
						
						  <text class="text" x="235" y="160">Structured ML Output</text>
						  <text class="text" x="235" y="176">(Labels, Trends)</text>
						
						  <text class="text" x="465" y="160">LLM Reasoning Layer</text>
						  <text class="text" x="465" y="176">(Explanation & Advice)</text>
						
						  <!-- Arrows -->
						  <line class="arrow" x1="220" y1="65" x2="250" y2="65"/>
						  <line class="arrow" x1="450" y1="65" x2="480" y2="65"/>
						  <line class="arrow" x1="580" y1="90" x2="335" y2="130"/>
						  <line class="arrow" x1="335" y1="155" x2="365" y2="155"/>
						
						</svg>



						<li><b>AI / LLM Layer.</b>
                            <p align="left">
                                 In VitalGuard AI, the LLM does not replace traditional machine learning models; instead,
                                 it augments them by providing contextual reasoning and natural-language interpretation.
                                 The LLM acts as a semantic layer that connects low-level physiological patterns identified
                                 by ML with user-facing insights and explanations.
                             </p>
							Cloud-based LLM services consume processed summaries and event flags from the backend to provide higher-level reasoning. They support:
							<ul>
								<li>Easy-to-understand health reports.</li>
								<li>Personalized lifestyle and habit recommendations.</li>
								<li>Context-aware explanations when anomalies are detected.</li>
								<li>Conversational query support for users asking about vitals or historical patterns.</li>
								<li>Privacy-preserving processing by sending only derived features, not raw biometric data.</li>
							</ul>
						</li>

						<li><b>Alerting &amp; User Interfaces.</b>
							When serious anomalies (e.g., potential arrhythmia, very high/low temperature) or
							a fall event are detected, the backend can trigger:
							<ul>
								<li>Notifications to configured emergency contacts.</li>
								<li>Potential escalation to emergency services depending on the deployment scenario.</li>
							</ul>
							Users and caregivers can access:
							<ul>
								<li>A web dashboard to view real-time vitals and historical trends.</li>
								<li>AI-generated health summaries and suggestions.</li>
								<li>Notifications and event logs (e.g., falls, critical alerts).</li>
							</ul>
						</li>
					</ul>

					

					<h3 align="left">Key Features</h3>

					<ul align="left">
						<li><b>Continuous Multi-Sensor Monitoring.</b>
							24/7 collection of heart rate, SpO₂, temperature, activity, and stress-related metrics.
						</li>
						<li><b>Real-Time Data Analytics.</b>
							On-device pre-processing and real-time streaming to a cloud server for further analytics.
						</li>
						<li><b>AI-Driven Health Reports.</b>
							LLM-generated health summaries and personalized suggestions, using structured features instead
							of raw time series.
						</li>
						<li><b>Emergency Alert System.</b>
							Automatic notifications when falls or critical vital sign anomalies are detected.
						</li>
						<li><b>Web Visualization Dashboard.</b>
							Browser-based UI to monitor real-time data, long-term trends, and AI-generated insights.
						</li>
					</ul>

					<h3 align="left">Technical Components</h3>

					<p align="left"><b>Hardware.</b></p>
					<ul align="left">
						<li>ESP32 microcontroller for edge computing and wireless connectivity.</li>
						<li>MAX86150: heart rate, SpO₂, ECG sensing.</li>
						<li>TMP117: high-accuracy temperature sensing.</li>
						<li>ADXL345: 3-axis accelerometer for motion and fall detection.</li>
						<li>FSR402: force/pressure sensing to infer contact and certain activities.</li>
						<li>Wi‑Fi: high-speed cloud connectivity.</li>
						<li>Bluetooth: short-range, low-energy link for local streaming or configuration.</li>
					</ul>
						 <a href="#" class="image fit">
                         <div class="img-box">
                         <img src="images/connection.jpg" alt="Case 3" />
                         </div>
                         </a>
				    </p>
					

					<p align="left"><b>Embedded / Edge Software.</b></p>
					<ul align="left">
						<li>MicroPython firmware running on the ESP32.</li>
						<li>Drivers and I²C/SPI/UART interfaces for integrated sensors.</li>
						<li>On-device pre-processing: simple filters, downsampling, feature extraction.</li>
						<li>Basic threshold-based anomaly hints and fall-detection heuristics.</li>
						<li>Wi‑Fi/Bluetooth communication management and reconnection logic.</li>
					</ul>

					<p align="left"><b>Cloud Backend &amp; Data Pipeline.</b></p>
					<ul align="left">
						<li>Google Cloud Platform (GCP) for hosting the backend.</li>
						<li>Python Flask server for REST APIs / socket endpoints.</li>
						<li>Socket-based or WebSocket data ingestion from the ESP32.</li>
						<li>Python scripts for feature extraction, normalization, and aggregation.</li>
						<li>Persistent storage for time-series vitals and event logs.</li>
						<li>Systemd + Gunicorn (or similar) for production deployment.</li>
					</ul>

					<p align="left"><b>AI / LLM Integration.</b></p>
					<ul align="left">
						<li>Third-party LLM accessed via API.</li>
						<li>Prompting with structured trends, statistics, and events rather than raw signals.</li>
						<li>Generation of human-friendly reports and explanations.</li>
						<li>Context-aware reasoning that combines vitals, motion, and historical baselines to improve anomaly interpretation.</li>
                        <li>LLM outputs integrated back into the dashboard and alerting pipeline for real-time guidance.</li>
					</ul>

					<p align="left"><b>Frontend &amp; User Experience.</b></p>
					<ul align="left">
						<li>Web UI built with HTML, CSS, and JavaScript.</li>
						<li>Visualization of current vitals, historical plots, and risk indicators.</li>
						<li>Display of AI-generated health summaries and recommendations.</li>
						<li>Mobile-friendly view; optional mobile app for richer interaction.</li>
					</ul>

					<h3 align="left">Project Structure</h3>

					<pre><code>.
                        ├── esp32/          # ESP32 (MicroPython) code
                        ├── gcp-server/     # GCP Flask backend service code
                        ├── docs/           # Project documentation
                        ├── .gitignore      # Git ignore configuration
                        └── README.md       # Project overview
					</code></pre>

					<h3 align="left">Prototype</h3>

					<p align="left">
						The initial prototype consists of an ESP32 development board connected to MAX86150, TMP117,
						ADXL345, and FSR402 sensors, assembled on a breadboard and later integrated into a wearable
						form factor (e.g., wristband or armband). The device is powered by a battery pack and streams
						data over Wi‑Fi to the GCP backend.
					</p>
                    <a href="#" class="image">
                      <img src="images/prototype_hardware1.jpg"
                        alt="VitalGuard AI hardware prototype"
                        style="max-width: 350px; height: auto;" />
                    </a>
					<a href="#" class="image">
                      <img src="images/prototype_hardware2.jpg"
                        alt="VitalGuard AI hardware prototype"
                        style="max-width: 350px; height: auto;" />
                    </a>
					<a href="#" class="image">
                      <img src="images/prototype_hardware3.jpg"
                        alt="VitalGuard AI hardware prototype"
                        style="max-width: 350px; height: auto;" />
                    </a>
					<p align="left">
						The web dashboard shows real-time heart rate, SpO₂, temperature, and activity indicators,
						along with AI-generated textual summaries. Fall events or critical vital anomalies are clearly
						highlighted as alerts in the UI.
					</p>
					<a href="#" class="image">
                      <img src="images/dashboard.jpg"
                        alt="VitalGuard AI hardware prototype"
                        style="max-width: 350px; height: auto;" />
                    </a>


	
				</div>
			</section>

			<!-- Results -->
			<section id="results" class="two">
				<div class="container">

					<header>
						<h2>Results</h2>
					</header>

					<p align="left">
						This section summarizes the experimental evaluation and demonstration of VitalGuard AI.
						We plan to present:
					</p>

					<ul align="left">
						<li><b>Real-Time Monitoring Demo.</b>
							Continuous streaming of heart rate, SpO₂, temperature, and motion data from the wearable
							to the web dashboard.
						</li>
                        <img src="images/report_scene.gif"
                             alt="Real-time monitoring demo animation"
                             style="max-width: 480px; width: 100%; margin: 8px 0 16px 0; display: block;" />
						<li><b>Abnormal Vital Detection.</b>
							Test scenarios where synthetic or controlled anomalies in heart rate or temperature trigger
							backend alerts.
						</li>
						<li><b>Fall Detection.</b>
							Evaluation of ADXL345-based fall detection heuristics and corresponding alert latency.
						</li>
						<li><b>AI Health Report Quality.</b>
							Qualitative examples of LLM-generated reports and user feedback on readability and usefulness.
						</li>
                        <li style="list-style-type: none;">
                            <video controls
                                   style="max-width: 480px; width: 100%; margin: 8px 0 16px 0; display: block;">
                                <source src="images/report_ui.mp4" type="video/mp4">
                            </video>
                        </li>
						<li><b>End-to-End Latency.</b>
							Measurement of the delay from sensing an event to visualizing it on the dashboard and/or
							receiving an alert notification.
						</li>
					</ul>
                        <p align="left">
                         Several demonstrations highlight the interaction between ML-based detection and LLM-based
                         explanation, showing how detected anomalies are transformed into human-readable health
                         reports and actionable recommendations.
                        </p>

					<!-- TODO: Replace with real result figures/plots/screenshots -->
					<article class="item">

                        <div style="display: flex; gap: 20px; align-items: flex-start;">

        <!-- 左边：running_ui -->
                            <div style="flex: 1; text-align: center;">
                                <a href="#" class="image fit">
                                    <img src="images/running_ui.gif" alt="Example VitalGuard AI results visualization">
                                </a>
                                <h4 style="margin-top: 10px;">Real-time Heart Rate & Activity Trend</h4>
                            </div>

        <!-- 右边：sick_ui_msgpush -->
                            <div style="flex: 1; text-align: center;">
                                <a href="#" class="image fit">
                                    <img src="images/sick_ui_msgpush.gif" alt="Sick UI Message Push Demo">
                                </a>
                                <h4 style="margin-top: 10px;">Health Status Alert Message Push</h4>
                            </div>

                        </div>

                    
				</div>

                 <br><br>
				<p align="left">
                Future work includes exploring more advanced LLM-based reasoning, where the model could
                support alert escalation decisions, such as determining when it is necessary to notify
                emergency contacts, while keeping final control within predefined safety rules.
                </p>
                  
			</section>

			<!-- References -->
			<section id="references" class="three">
				<div class="container">

					<header>
						<h2>References</h2>
					</header>

					<p align="left">
						Selected references and resources used in the design and implementation of VitalGuard AI:
					</p>

					<ul align="left">
						<li>MAX86150: Integrated PPG and ECG Analog Front End for Wearable Health – Product datasheet.</li>
						<li>TMP117: ±0.1&nbsp;°C High-Accuracy Digital Temperature Sensor – Product datasheet.</li>
						<li>ADXL345: 3-Axis, ±2&nbsp;g/±4&nbsp;g/±8&nbsp;g/±16&nbsp;g Digital Accelerometer – Product datasheet.</li>
						<li>FSR402: Force Sensitive Resistor – Product datasheet and application notes.</li>
						<li>MicroPython documentation: <a href="https://docs.micropython.org/" target="_blank">https://docs.micropython.org/</a></li>
						<li>Flask documentation: <a href="https://flask.palletsprojects.com/" target="_blank">https://flask.palletsprojects.com/</a></li>
						<li>Google Cloud Platform (GCP) documentation: <a href="https://cloud.google.com/docs" target="_blank">https://cloud.google.com/docs</a></li>
						<li>LLM / AI API provider documentation (e.g., OpenAI, Anthropic, etc.) for health report generation.</li>
					</ul>

				</div>
			</section>

			<!-- Our Team -->
			<section id="team" class="two">
				<div class="container">

					<header>
						<h2>Our Team</h2>
					</header>

					<!-- <a href="#" class="image featured"><img src="images/pic08.jpg" alt="" /></a> -->

					<div class="row">
						<div class="4u 12u$(mobile)">
							<article class="item">
								<a href="#" class="image fit"><img src="images/yt2992.jpg" alt="Team member 1" /></a>
								<header>
									<h3>Team Member 1</h3>
									<p>Yizheng Tang / yt2992 – Embedded systems &amp; firmware</p>
								</header>
							</article>
						</div>
						<div class="4u 12u$(mobile)">
							<article class="item">
								<!-- TODO: Replace with Team Member 2 photo -->
								<a href="#" class="image fit"><img src="images/pic07.jpg" alt="Team member 2" /></a>
								<header>
									<h3>Team Member 2</h3>
									<p>Hao Chen / hc3625 – Cloud backend frontend &amp; data pipeline</p>
								</header>
							</article>
						</div>
                        <div class="4u 12u$(mobile)">
							<article class="item">
								<!-- TODO: Replace with Team Member 3 photo -->
								<a href="#" class="image fit"><img src="images/pic07.jpg" alt="Team member 1" /></a>
								<header>
									<h3>Team Member 3</h3>
									<p>Sripad Karne / sk5695 – Machine Learning &amp; Data digging</p>
								</header>
							</article>
						</div>
						<div class="4u$ 12u$(mobile)">
							<article class="item">
								<!-- TODO: Replace with Team Member 4 photo -->
								<a href="#" class="image fit"><img src="images/pic_Daolin.jpg" alt="Team member 3" /></a>
								<header>
									<h3>Team Member 4</h3>
									<p>Daolin Li / dl3832 – AI integration</p>
								</header>
							</article>
						</div>
					</div>

				</div>
			</section>

			<!-- Contact -->
			<section id="contact" class="four">
				<div class="container">

					<header>
						<h2>Contact</h2>
					</header>

					<p align="left">
						<!-- TODO: Fill in real contact names and emails -->
						<strong>Contact Name 1:</strong> Daolin Li (Uni: dl3832) <a href="mailto:dl3832@columbia.edu">dl3832@columbia.edu</a><br/>
                        <strong>Contact Name 2:</strong> Hao Chen (Uni: hc3625) <a href="mailto:hc3625@columbia.edu">hc3625@columbia.edu</a><br/>
                        <strong>Contact Name 3:</strong> Sripad Karne (Uni: sk5695) <a href="mailto:sk5695@columbia.edu">sk5695@columbia.edu</a><br/>
                        <strong>Contact Name 4:</strong> Yizheng Tang (Uni: yt2992) <a href="mailto:yt2992@columbia.edu">yt2992@columbia.edu</a><br/>
                        <br>
                        <strong>Other Contact Information:</strong>
                        <a href="https://github.com/JarrettChen217/EECS-E4764-2025-Fall-Final-Project-vitalguard-ai" target="_blank">GitHub Repository</a>,
                        <a href="https://api.vitalguardhealth.dev/" target="_blank">Project Website</a><br/>
                        <br/>
						<strong>Columbia University </strong>
						<a href="http://www.ee.columbia.edu" target="_blank">Department of Electrical Engineering</a><br/>
						<!-- <strong>Class Website:</strong>
							<a href="https://edblogs.columbia.edu/eecs4764-001-2019-3/">Columbia University EECS E4764 Fall '22 IoT</a><br/> -->
						<strong>Instructor:</strong>
						<a href="https://www.engineering.columbia.edu/faculty-staff/directory/xiaofan-fred-jiang" target="_blank">
							Professor Xiaofan (Fred) Jiang
						</a>
					</p>

					<!-- Contact form is disabled for this static project website
					<form method="post" action="#">
						<div class="row">
							<div class="6u 12u$(mobile)"><input type="text" name="name" placeholder="Name" /></div>
							<div class="6u$ 12u$(mobile)"><input type="text" name="email" placeholder="Email" /></div>
							<div class="12u$">
								<textarea name="message" placeholder="Message"></textarea>
							</div>
							<div class="12u$">
								<input type="submit" value="Send Message" />
							</div>
						</div>
					</form>
					-->

				</div>
			</section>

		</div>

		<!-- Footer -->
		<div id="footer">

			<!-- Copyright -->
			<ul class="copyright">
				<li>&copy; VitalGuard AI IoT Project | All rights reserved.</li>
				<li>Design: <a href="http://html5up.net" target="_blank">HTML5 UP</a></li>
			</ul>

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/jquery.scrollzer.min.js"></script>
		<script src="assets/js/skel.min.js"></script>
		<script src="assets/js/util.js"></script>
		<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
		<script src="assets/js/main.js"></script>

	</body>
</html>
